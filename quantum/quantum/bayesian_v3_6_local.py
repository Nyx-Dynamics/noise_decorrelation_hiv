#!/usr/bin/env python3
"""
Bayesian Model v3.6 - EXPANDED WITH RATIO DATA
Including Valcour (absolute) + Young + Sailasuta (ratios converted to absolute)

Updated: November 16, 2025
Investigator: AC (Nyx Dynamics LLC)
"""

import numpy as np
import pandas as pd
import pymc as pm
import arviz as az
import matplotlib.pyplot as plt
from scipy import stats
import argparse
from pathlib import Path
import warnings
import sys

warnings.filterwarnings('ignore')

print("\n" + "â•”" + "=" * 78 + "â•—")
print("â•‘" + " " * 15 + "BAYESIAN MODEL v3.6 - WITH RATIO DATA" + " " * 22 + "â•‘")
print("â•‘" + " " * 78 + "â•‘")
print("â•‘" + " Valcour (absolute) + Young + Sailasuta (ratios â†’ absolute)" + " " * 14 + "â•‘")
print("â•š" + "=" * 78 + "â•\n")

# ============================================================================
# FILE PATH CONFIGURATION
# ============================================================================

# Auto-detect if we're running locally or on Claude's system
script_dir = Path(__file__).resolve().parent

# Try to find the data directory
possible_data_dirs = [
    script_dir / "data" / "curated",  # If script is in quantum/quantum/
    script_dir.parent / "data" / "curated",  # If script is in quantum/
    script_dir.parent.parent / "data" / "curated",  # One more level up
    Path("/mnt/user-data/uploads"),  # Claude's system
]

data_dir = None
for possible_dir in possible_data_dirs:
    if possible_dir.exists():
        data_dir = possible_dir
        print(f"âœ… Found data directory: {data_dir}")
        break

if data_dir is None:
    print("âŒ Could not find data directory. Please set manually.")
    print("Looking for:")
    for pd in possible_data_dirs:
        print(f"  - {pd}")
    sys.exit(1)

# Define file paths
VALCOUR_DIR = data_dir / "absolute"
RATIO_DIR = data_dir / "ratio"

VALCOUR_FILES = [
    VALCOUR_DIR / "valcour_2015_week_0.xlsx",
    VALCOUR_DIR / "valcour_2015_week_4.xlsx",
    VALCOUR_DIR / "valcour_2015_week_12.xlsx",
    VALCOUR_DIR / "valcour_2015_week_24.xlsx",
]

YOUNG_FILE = RATIO_DIR / "YOUNG_2014_CROSS_SECTIONAL_DATA.csv"
SAILASUTA_FILE = RATIO_DIR / "Sailasuta_2012.csv"
CHANG_FILE = data_dir / "absolute" / "CHANG_2002_EXTRACTED.csv"

print(f"\nðŸ“‚ Data paths configured:")
print(f"   Absolute data: {VALCOUR_DIR}")
print(f"   Ratio data: {RATIO_DIR}")

# ============================================================================
# REFERENCE CREATINE VALUES FOR RATIO CONVERSION
# ============================================================================
CR_REFERENCE = {
    'BG': 8.0,  # Basal Ganglia
    'FWM': 6.8,  # Frontal White Matter
    'PGM': 7.8,  # Posterior Gray Matter / Parietal
    'FGM': 7.8,  # Frontal Gray Matter
    'AC': 7.8,  # Anterior Cingulate
    'OGM': 7.5  # Occipital Gray Matter
}


def convert_ratio_to_absolute(ratio_value, region):
    """Convert metabolite/Cr ratio to absolute concentration (mM)"""
    cr_ref = CR_REFERENCE.get(region, 7.5)
    return ratio_value * cr_ref


# ============================================================================
# LOAD DATA
# ============================================================================

print("\n" + "=" * 80)
print("LOADING DATA")
print("=" * 80)

# ------------------------------
# Load Valcour data (absolute concentrations)
# ------------------------------
print("\nðŸ“ Loading Valcour 2015 data (absolute concentrations)...")

valcour_dfs = []
for week_file in VALCOUR_FILES:
    if week_file.exists():
        try:
            df = pd.read_excel(str(week_file))
            week_name = week_file.stem.replace('valcour_2015_', '')
            df['Week'] = week_name
            valcour_dfs.append(df)
            print(f"   âœ… Loaded {week_name}: {len(df)} patients")
        except Exception as e:
            print(f"   âš  Error reading {week_file.name}: {e}")
    else:
        print(f"   âš  File not found: {week_file.name}")

if valcour_dfs:
    valcour_df = pd.concat(valcour_dfs, ignore_index=True)
    print(f"\nâœ… Total Valcour records: {len(valcour_df)}")
else:
    print("   âš  No Valcour data loaded")
    valcour_df = pd.DataFrame()

# ------------------------------
# Load Young 2014 data (ratios)
# ------------------------------
print("\nðŸ“ Loading Young 2014 data (ratios)...")

if YOUNG_FILE.exists():
    try:
        young_df = pd.read_csv(str(YOUNG_FILE))

        # Parse the ratio data - extract NAA and Cho
        young_processed = []

        for _, row in young_df.iterrows():
            if row['Metabolite'] == 'NAA/Cr':
                # Convert ratio to absolute using median value
                naa_abs = convert_ratio_to_absolute(row['Ratio_Median'], row['Region'])

                young_processed.append({
                    'Study': 'Young_2014',
                    'Phase': 'Acute' if row['Phase'] == 'Primary' else row['Phase'],
                    'Region': row['Region'],
                    'n': row['n'],
                    'NAA': naa_abs,
                    'NAA_SE': convert_ratio_to_absolute(row['SE'], row['Region']),
                    'Source': 'Ratio'
                })
            elif row['Metabolite'] == 'Cho/Cr':
                # Find matching NAA entry to update
                for proc in young_processed:
                    if (proc['Region'] == row['Region'] and
                            proc['Phase'] == ('Acute' if row['Phase'] == 'Primary' else row['Phase']) and
                            proc['n'] == row['n']):
                        cho_abs = convert_ratio_to_absolute(row['Ratio_Median'], row['Region'])
                        proc['Cho'] = cho_abs
                        proc['Cho_SE'] = convert_ratio_to_absolute(row['SE'], row['Region'])
                        break

        young_clean = pd.DataFrame(young_processed)
        print(f"âœ… Loaded Young 2014: {len(young_clean)} group means")
        print(f"   Phases: {young_clean['Phase'].value_counts().to_dict()}")
        print(f"   Regions: {young_clean['Region'].value_counts().to_dict()}")

    except Exception as e:
        print(f"âš  Error loading Young 2014: {e}")
        young_clean = pd.DataFrame()
else:
    print(f"âš  File not found: {YOUNG_FILE}")
    young_clean = pd.DataFrame()

# ------------------------------
# Load Sailasuta 2012 data (ratios)
# ------------------------------
print("\nðŸ“ Loading Sailasuta 2012 data (ratios)...")

if SAILASUTA_FILE.exists():
    try:
        sailasuta_df = pd.read_csv(str(SAILASUTA_FILE), skiprows=1)

        # Process Sailasuta data
        sailasuta_processed = []

        for phase in sailasuta_df['Group'].unique():
            for region in sailasuta_df['Brain_Region'].unique():
                phase_region_data = sailasuta_df[
                    (sailasuta_df['Group'] == phase) &
                    (sailasuta_df['Brain_Region'] == region)
                    ]

                naa_row = phase_region_data[phase_region_data['Metabolite'] == 'NAA']
                cho_row = phase_region_data[phase_region_data['Metabolite'] == 'tCho']

                if not naa_row.empty:
                    naa_value = naa_row['Value'].values[0]
                    naa_sd = naa_row['SD'].values[0]
                    n = int(naa_row['n'].values[0])

                    naa_abs = convert_ratio_to_absolute(naa_value, region)
                    naa_se = convert_ratio_to_absolute(naa_sd / np.sqrt(n), region)

                    entry = {
                        'Study': 'Sailasuta_2012',
                        'Phase': phase,
                        'Region': region,
                        'n': n,
                        'NAA': naa_abs,
                        'NAA_SE': naa_se,
                        'Source': 'Ratio'
                    }

                    if not cho_row.empty:
                        cho_value = cho_row['Value'].values[0]
                        cho_sd = cho_row['SD'].values[0]
                        cho_abs = convert_ratio_to_absolute(cho_value, region)
                        cho_se = convert_ratio_to_absolute(cho_sd / np.sqrt(n), region)
                        entry['Cho'] = cho_abs
                        entry['Cho_SE'] = cho_se

                    sailasuta_processed.append(entry)

        sailasuta_clean = pd.DataFrame(sailasuta_processed)
        print(f"âœ… Loaded Sailasuta 2012: {len(sailasuta_clean)} group means")
        print(f"   Phases: {sailasuta_clean['Phase'].value_counts().to_dict()}")
        print(f"   Regions: {sailasuta_clean['Region'].value_counts().to_dict()}")

    except Exception as e:
        print(f"âš  Error loading Sailasuta 2012: {e}")
        sailasuta_clean = pd.DataFrame()
else:
    print(f"âš  File not found: {SAILASUTA_FILE}")
    sailasuta_clean = pd.DataFrame()

# ------------------------------
# Load Chang 2002 reference data (absolute)
# ------------------------------
print("\nðŸ“ Loading Chang 2002 reference data...")

if CHANG_FILE.exists():
    try:
        chang_df = pd.read_csv(str(CHANG_FILE))
        print(f"âœ… Loaded Chang 2002: {len(chang_df)} measurements")
    except Exception as e:
        print(f"âš  Error loading Chang 2002: {e}")
        chang_df = pd.DataFrame()
else:
    print(f"âš  File not found: {CHANG_FILE}")
    chang_df = pd.DataFrame()

# ============================================================================
# PREPARE DATA FOR BASAL GANGLIA (PRIMARY FOCUS)
# ============================================================================

print("\n" + "=" * 80)
print("PREPARING BASAL GANGLIA DATA")
print("=" * 80)

# ------------------------------
# Valcour: Individual acute patients
# ------------------------------
if not valcour_df.empty:
    # Use week_0 (baseline) for acute data
    valcour_bg_acute = valcour_df[
        (valcour_df['Week'] == 'week_0') &
        (valcour_df['NAA'].notna()) &
        (valcour_df['Cho'].notna())
        ].copy()

    naa_obs_acute_valcour = valcour_bg_acute['NAA'].values
    cho_obs_acute_valcour = valcour_bg_acute['Cho'].values
    n_acute_valcour = len(naa_obs_acute_valcour)

    print(f"\nðŸ“Š VALCOUR ACUTE (Individual Patients, n={n_acute_valcour}):")
    print(f"   NAA: {naa_obs_acute_valcour.mean():.2f} Â± {naa_obs_acute_valcour.std():.2f} mM")
    print(f"   Cho: {cho_obs_acute_valcour.mean():.2f} Â± {cho_obs_acute_valcour.std():.2f} mM")
else:
    naa_obs_acute_valcour = np.array([])
    cho_obs_acute_valcour = np.array([])
    n_acute_valcour = 0
    print("\nâš  No Valcour acute data available")

# ------------------------------
# Young: Group means for BG
# ------------------------------
if not young_clean.empty:
    young_bg = young_clean[young_clean['Region'] == 'BG'].copy()

    young_bg_acute = young_bg[young_bg['Phase'] == 'Acute']
    young_bg_chronic = young_bg[young_bg['Phase'] == 'Chronic']
    young_bg_control = young_bg[young_bg['Phase'] == 'Control']

    print(f"\nðŸ“Š YOUNG 2014 BG:")
    if not young_bg_acute.empty:
        print(f"   Acute: n={young_bg_acute['n'].values[0]}, NAA={young_bg_acute['NAA'].values[0]:.2f} mM")
    if not young_bg_chronic.empty:
        print(f"   Chronic: n={young_bg_chronic['n'].values[0]}, NAA={young_bg_chronic['NAA'].values[0]:.2f} mM")
    if not young_bg_control.empty:
        print(f"   Control: n={young_bg_control['n'].values[0]}, NAA={young_bg_control['NAA'].values[0]:.2f} mM")
else:
    young_bg_acute = pd.DataFrame()
    young_bg_chronic = pd.DataFrame()
    young_bg_control = pd.DataFrame()

# ------------------------------
# Sailasuta: Group means for BG
# ------------------------------
if not sailasuta_clean.empty:
    sailasuta_bg = sailasuta_clean[sailasuta_clean['Region'] == 'BG'].copy()

    sailasuta_bg_acute = sailasuta_bg[sailasuta_bg['Phase'] == 'Acute']
    sailasuta_bg_chronic = sailasuta_bg[sailasuta_bg['Phase'] == 'Chronic']
    sailasuta_bg_control = sailasuta_bg[sailasuta_bg['Phase'] == 'Control']

    print(f"\nðŸ“Š SAILASUTA 2012 BG:")
    if not sailasuta_bg_acute.empty:
        print(f"   Acute: n={sailasuta_bg_acute['n'].values[0]}, NAA={sailasuta_bg_acute['NAA'].values[0]:.2f} mM")
    if not sailasuta_bg_chronic.empty:
        print(
            f"   Chronic: n={sailasuta_bg_chronic['n'].values[0]}, NAA={sailasuta_bg_chronic['NAA'].values[0]:.2f} mM")
    if not sailasuta_bg_control.empty:
        print(
            f"   Control: n={sailasuta_bg_control['n'].values[0]}, NAA={sailasuta_bg_control['NAA'].values[0]:.2f} mM")
else:
    sailasuta_bg_acute = pd.DataFrame()
    sailasuta_bg_chronic = pd.DataFrame()
    sailasuta_bg_control = pd.DataFrame()

# ------------------------------
# Combine data for modeling
# ------------------------------

# ACUTE: Combine Valcour individuals + Young/Sailasuta group means
acute_naa_list = [naa_obs_acute_valcour] if n_acute_valcour > 0 else []
acute_cho_list = [cho_obs_acute_valcour] if n_acute_valcour > 0 else []

# Add Young acute as weighted observations (repeat by sample size)
if not young_bg_acute.empty:
    n_young = int(young_bg_acute['n'].values[0])
    naa_young = young_bg_acute['NAA'].values[0]
    se_young = young_bg_acute['NAA_SE'].values[0]
    # Create pseudo-observations with appropriate spread
    np.random.seed(42)
    acute_naa_list.append(np.random.normal(naa_young, se_young * np.sqrt(n_young), n_young))

    if 'Cho' in young_bg_acute.columns and not pd.isna(young_bg_acute['Cho'].values[0]):
        cho_young = young_bg_acute['Cho'].values[0]
        cho_se_young = young_bg_acute['Cho_SE'].values[0]
        acute_cho_list.append(np.random.normal(cho_young, cho_se_young * np.sqrt(n_young), n_young))

# Add Sailasuta acute
if not sailasuta_bg_acute.empty:
    n_sail = int(sailasuta_bg_acute['n'].values[0])
    naa_sail = sailasuta_bg_acute['NAA'].values[0]
    se_sail = sailasuta_bg_acute['NAA_SE'].values[0]
    np.random.seed(43)
    acute_naa_list.append(np.random.normal(naa_sail, se_sail * np.sqrt(n_sail), n_sail))

    if 'Cho' in sailasuta_bg_acute.columns and not pd.isna(sailasuta_bg_acute['Cho'].values[0]):
        cho_sail = sailasuta_bg_acute['Cho'].values[0]
        cho_se_sail = sailasuta_bg_acute['Cho_SE'].values[0]
        acute_cho_list.append(np.random.normal(cho_sail, cho_se_sail * np.sqrt(n_sail), n_sail))

naa_obs_acute = np.concatenate(acute_naa_list) if acute_naa_list else np.array([])
cho_obs_acute = np.concatenate(acute_cho_list) if acute_cho_list else np.array([])

# CHRONIC: Combine group means
chronic_naa_list = []
chronic_cho_list = []

if not young_bg_chronic.empty:
    chronic_naa_list.append(young_bg_chronic['NAA'].values)
    if 'Cho' in young_bg_chronic.columns and not pd.isna(young_bg_chronic['Cho'].values[0]):
        chronic_cho_list.append(young_bg_chronic['Cho'].values)

if not sailasuta_bg_chronic.empty:
    chronic_naa_list.append(sailasuta_bg_chronic['NAA'].values)
    if 'Cho' in sailasuta_bg_chronic.columns and not pd.isna(sailasuta_bg_chronic['Cho'].values[0]):
        chronic_cho_list.append(sailasuta_bg_chronic['Cho'].values)

naa_obs_chronic = np.concatenate(chronic_naa_list) if chronic_naa_list else np.array([8.79])
cho_obs_chronic = np.concatenate(chronic_cho_list) if chronic_cho_list else np.array([2.40])

# CONTROL: Use literature reference + any group means
if not chang_df.empty:
    chang_bg_control = chang_df[(chang_df['Phase'] == 'Control') & (chang_df['Region'] == 'BG')]
    if not chang_bg_control.empty:
        chang_naa = chang_bg_control[chang_bg_control['Metabolite'] == 'NAA']['Mean'].values
        chang_cho = chang_bg_control[chang_bg_control['Metabolite'] == 'Cho']['Mean'].values
        naa_obs_control = chang_naa if len(chang_naa) > 0 else np.array([9.55])
        cho_obs_control = chang_cho if len(chang_cho) > 0 else np.array([2.18])
    else:
        naa_obs_control = np.array([9.55])
        cho_obs_control = np.array([2.18])
else:
    naa_obs_control = np.array([9.55])
    cho_obs_control = np.array([2.18])

print("\n" + "=" * 80)
print("FINAL COMBINED DATA FOR MODEL")
print("=" * 80)

if len(naa_obs_acute) > 0:
    print(f"\nâœ… ACUTE: n={len(naa_obs_acute)}")
    print(f"   NAA: {naa_obs_acute.mean():.2f} Â± {naa_obs_acute.std():.2f} mM")
    if len(cho_obs_acute) > 0:
        print(f"   Cho: {cho_obs_acute.mean():.2f} Â± {cho_obs_acute.std():.2f} mM")
else:
    print(f"\nâš  ACUTE: n=0 (no data available)")

print(f"\nâœ… CHRONIC: n={len(naa_obs_chronic)}")
print(f"   NAA: {naa_obs_chronic.mean():.2f} Â± {naa_obs_chronic.std():.2f} mM")
if len(cho_obs_chronic) > 0:
    print(f"   Cho: {cho_obs_chronic.mean():.2f} Â± {cho_obs_chronic.std():.2f} mM")

print(f"\nâœ… CONTROL: n={len(naa_obs_control)}")
print(f"   NAA: {naa_obs_control.mean():.2f} mM")
print(f"   Cho: {cho_obs_control.mean():.2f} mM")

# Abort if no acute data
if len(naa_obs_acute) == 0:
    print("\nâŒ ERROR: No acute data available for modeling!")
    print("   Please check that data files are in the correct location.")
    sys.exit(1)

# ============================================================================
# MODEL PARAMETERS
# ============================================================================

print("\n" + "=" * 80)
print("MODEL PARAMETERS")
print("=" * 80)

# Microtubule parameters
L_MT = 2000e-9  # Microtubule length (m)
k_B = 1.38e-23  # Boltzmann constant
T = 310  # Temperature (K)
hbar = 1.055e-34  # Reduced Planck constant

# Enzyme kinetics parameters
V_max_base = 100.0  # Base enzyme velocity (nmol/min/mg)
K_m = 50.0  # Michaelis constant (Î¼M)
S_0 = 100.0  # Substrate concentration (Î¼M)

print("\nðŸ”¬ Physical Constants:")
print(f"   Microtubule length: {L_MT * 1e9:.0f} nm")
print(f"   Temperature: {T} K")
print(f"   Enzyme V_max baseline: {V_max_base} nmol/min/mg")

# ============================================================================
# BAYESIAN MODEL v3.6
# ============================================================================

print("\n" + "=" * 80)
print("BUILDING BAYESIAN MODEL v3.6")
print("=" * 80)

with pm.Model() as model:
    # ========================================================================
    # PRIORS: Noise correlation lengths
    # ========================================================================

    Î¾_acute = pm.TruncatedNormal('Î¾_acute', mu=0.6, sigma=0.1,
                                 lower=0.3, upper=0.9)

    Î¾_chronic = pm.TruncatedNormal('Î¾_chronic', mu=0.8, sigma=0.1,
                                   lower=0.5, upper=1.2)

    Î¾_control = pm.TruncatedNormal('Î¾_control', mu=0.5, sigma=0.05,
                                   lower=0.3, upper=0.7)

    # ========================================================================
    # PROTECTION MECHANISM
    # ========================================================================

    Î²_Î¾ = pm.TruncatedNormal('Î²_Î¾', mu=-2.0, sigma=0.5, lower=-4.0, upper=0.0)


    def quantum_protection_factor(Î¾, L_MT, Î²_Î¾):
        """Quantum coherence protection: Î“_eff = exp(Î²_Î¾ * Î¾/L_MT)"""
        return pm.math.exp(Î²_Î¾ * (Î¾ * 1e-9) / L_MT)


    Î“_acute = quantum_protection_factor(Î¾_acute, L_MT, Î²_Î¾)
    Î“_chronic = quantum_protection_factor(Î¾_chronic, L_MT, Î²_Î¾)
    Î“_control = quantum_protection_factor(Î¾_control, L_MT, Î²_Î¾)

    # ========================================================================
    # METABOLIC MODEL
    # ========================================================================

    # Baseline NAA synthesis rate (control)
    r_NAA_baseline = pm.TruncatedNormal('r_NAA_baseline', mu=9.5, sigma=0.3,
                                        lower=8.0, upper=11.0)

    # Phase-specific modulation
    Î±_acute = pm.TruncatedNormal('Î±_acute', mu=1.0, sigma=0.1,
                                 lower=0.8, upper=1.3)
    Î±_chronic = pm.TruncatedNormal('Î±_chronic', mu=0.92, sigma=0.05,
                                   lower=0.8, upper=1.0)

    # Predicted NAA
    NAA_control_mean = r_NAA_baseline
    NAA_acute_mean = r_NAA_baseline * Î±_acute * Î“_acute
    NAA_chronic_mean = r_NAA_baseline * Î±_chronic * Î“_chronic

    # Store for posterior analysis
    NAA_control_pred = pm.Deterministic('NAA_control_mean', NAA_control_mean)
    NAA_acute_pred = pm.Deterministic('NAA_acute_mean', NAA_acute_mean)
    NAA_chronic_pred = pm.Deterministic('NAA_chronic_mean', NAA_chronic_mean)

    # ========================================================================
    # CHOLINE MODEL
    # ========================================================================

    Cho_baseline = pm.TruncatedNormal('Cho_baseline', mu=2.2, sigma=0.1,
                                      lower=1.8, upper=2.6)

    # Inflammation increases Cho
    Cho_control_mean = Cho_baseline
    Cho_acute_mean = Cho_baseline * 1.15  # Acute inflammation
    Cho_chronic_mean = Cho_baseline * 1.10  # Chronic inflammation

    # ========================================================================
    # LIKELIHOOD
    # ========================================================================

    # Individual-level acute observations
    Ïƒ_naa_acute = pm.HalfNormal('Ïƒ_naa_acute', sigma=0.5)
    Ïƒ_cho_acute = pm.HalfNormal('Ïƒ_cho_acute', sigma=0.3)

    if len(naa_obs_acute) > 0:
        NAA_acute_obs = pm.Normal('NAA_acute_obs',
                                  mu=NAA_acute_mean,
                                  sigma=Ïƒ_naa_acute,
                                  observed=naa_obs_acute)

    if len(cho_obs_acute) > 0:
        Cho_acute_obs = pm.Normal('Cho_acute_obs',
                                  mu=Cho_acute_mean,
                                  sigma=Ïƒ_cho_acute,
                                  observed=cho_obs_acute)

    # Group-level chronic observations
    if len(naa_obs_chronic) > 0:
        NAA_chronic_obs = pm.Normal('NAA_chronic_obs',
                                    mu=NAA_chronic_mean,
                                    sigma=0.3,
                                    observed=naa_obs_chronic)

    if len(cho_obs_chronic) > 0:
        Cho_chronic_obs = pm.Normal('Cho_chronic_obs',
                                    mu=Cho_chronic_mean,
                                    sigma=0.2,
                                    observed=cho_obs_chronic)

    # Control observations
    NAA_control_obs = pm.Normal('NAA_control_obs',
                                mu=NAA_control_mean,
                                sigma=0.2,
                                observed=naa_obs_control)

    Cho_control_obs = pm.Normal('Cho_control_obs',
                                mu=Cho_control_mean,
                                sigma=0.2,
                                observed=cho_obs_control)

    # ========================================================================
    # HYPOTHESIS TEST
    # ========================================================================

    Î”Î¾ = pm.Deterministic('Î”Î¾', Î¾_chronic - Î¾_acute)

print("\nâœ… Model built successfully!")
print(f"   Free parameters: {len(model.free_RVs)}")
print(f"   Observations: {len(naa_obs_acute)} acute + {len(naa_obs_chronic)} chronic + {len(naa_obs_control)} control")

# ============================================================================
# INFERENCE
# ============================================================================

print("\n" + "=" * 80)
print("RUNNING MCMC INFERENCE")
print("=" * 80)

with model:
    print("\nâ³ Sampling (this may take 5-10 minutes)...")
    trace = pm.sample(
        draws=2000,
        tune=1000,
        chains=4,
        cores=4,
        target_accept=0.98,
        return_inferencedata=True,
        random_seed=42
    )

    print("\nâœ… Sampling complete!")

    print("\nâ³ Generating posterior predictive samples...")
    ppc = pm.sample_posterior_predictive(trace, random_seed=42)
    print("âœ… Posterior predictive complete!")

# ============================================================================
# RESULTS
# ============================================================================

print("\n" + "=" * 80)
print("BAYESIAN INFERENCE RESULTS")
print("=" * 80)

posterior = trace.posterior

# Key parameters
Î¾_acute_samples = posterior['Î¾_acute'].values.flatten()
Î¾_chronic_samples = posterior['Î¾_chronic'].values.flatten()
Î”Î¾_samples = posterior['Î”Î¾'].values.flatten()
Î²_Î¾_samples = posterior['Î²_Î¾'].values.flatten()

print("\nðŸ”¬ CORRELATION LENGTH PARAMETERS:")
print(f"\n   Î¾_acute = {Î¾_acute_samples.mean():.3f} Â± {Î¾_acute_samples.std():.3f} nm")
print(f"   95% HDI: [{np.percentile(Î¾_acute_samples, 2.5):.3f}, {np.percentile(Î¾_acute_samples, 97.5):.3f}]")

print(f"\n   Î¾_chronic = {Î¾_chronic_samples.mean():.3f} Â± {Î¾_chronic_samples.std():.3f} nm")
print(f"   95% HDI: [{np.percentile(Î¾_chronic_samples, 2.5):.3f}, {np.percentile(Î¾_chronic_samples, 97.5):.3f}]")

print(f"\n   Î”Î¾ = Î¾_chronic - Î¾_acute = {Î”Î¾_samples.mean():.3f} Â± {Î”Î¾_samples.std():.3f} nm")
print(f"   95% HDI: [{np.percentile(Î”Î¾_samples, 2.5):.3f}, {np.percentile(Î”Î¾_samples, 97.5):.3f}]")

# Hypothesis test
P_acute_shorter = (Î”Î¾_samples > 0).sum() / len(Î”Î¾_samples)
print(f"\nâœ… P(Î¾_acute < Î¾_chronic) = {P_acute_shorter:.4f}")

if P_acute_shorter > 0.999:
    print("   *** HYPOTHESIS STRONGLY SUPPORTED ***")
elif P_acute_shorter > 0.99:
    print("   *** HYPOTHESIS SUPPORTED ***")
elif P_acute_shorter > 0.95:
    print("   ** HYPOTHESIS LIKELY **")

print(f"\nðŸ”¬ PROTECTION FACTOR EXPONENT:")
print(f"   Î²_Î¾ = {Î²_Î¾_samples.mean():.2f} Â± {Î²_Î¾_samples.std():.2f}")

# Predicted NAA
naa_acute_pred = posterior['NAA_acute_mean'].values.flatten()
naa_chronic_pred = posterior['NAA_chronic_mean'].values.flatten()
naa_control_pred = posterior['NAA_control_mean'].values.flatten()

print(f"\nðŸ“Š PREDICTED vs OBSERVED NAA:")
print(f"\n   Acute: {naa_acute_pred.mean():.2f} mM (pred) vs {naa_obs_acute.mean():.2f} mM (obs)")
print(f"   Error: {abs(naa_acute_pred.mean() - naa_obs_acute.mean()) / naa_obs_acute.mean() * 100:.1f}%")

print(f"\n   Chronic: {naa_chronic_pred.mean():.2f} mM (pred) vs {naa_obs_chronic.mean():.2f} mM (obs)")
print(f"   Error: {abs(naa_chronic_pred.mean() - naa_obs_chronic.mean()) / naa_obs_chronic.mean() * 100:.1f}%")

print(f"\n   Control: {naa_control_pred.mean():.2f} mM (pred) vs {naa_obs_control.mean():.2f} mM (obs)")
print(f"   Error: {abs(naa_control_pred.mean() - naa_obs_control.mean()) / naa_obs_control.mean() * 100:.1f}%")

# ============================================================================
# CONVERGENCE DIAGNOSTICS
# ============================================================================

print("\n" + "=" * 80)
print("CONVERGENCE DIAGNOSTICS")
print("=" * 80)

# R-hat (should be < 1.01)
rhat = az.rhat(trace)
rhat_max = float(rhat.max().values) if hasattr(rhat.max(), 'values') else float(rhat.max())

print(f"\n   Max R-hat: {rhat_max:.4f}")
if rhat_max < 1.01:
    print("   âœ… Excellent convergence (R-hat < 1.01)")
elif rhat_max < 1.05:
    print("   âœ… Good convergence (R-hat < 1.05)")
else:
    print("   âš  May need more samples (R-hat > 1.05)")

# Effective sample size
ess = az.ess(trace)
ess_min = float(ess.min().values) if hasattr(ess.min(), 'values') else float(ess.min())

print(f"\n   Min ESS: {ess_min:.0f}")
if ess_min > 400:
    print("   âœ… Adequate effective sample size")
else:
    print("   âš  Low ESS, consider more samples")

# ============================================================================
# SAVE RESULTS
# ============================================================================

print("\n" + "=" * 80)
print("SAVING RESULTS")
print("=" * 80)

# Create output directory
output_dir = script_dir / "results_v3_6"
output_dir.mkdir(exist_ok=True)

# Save summary statistics
summary_path = output_dir / 'summary.csv'
summary_df = az.summary(trace, hdi_prob=0.95)
summary_df.to_csv(str(summary_path))
print(f"âœ… Saved summary: {summary_path}")

# Create results summary
results_summary = {
    'Parameter': ['Î¾_acute', 'Î¾_chronic', 'Î”Î¾', 'Î²_Î¾', 'P(Î¾_acute < Î¾_chronic)'],
    'Mean': [
        Î¾_acute_samples.mean(),
        Î¾_chronic_samples.mean(),
        Î”Î¾_samples.mean(),
        Î²_Î¾_samples.mean(),
        P_acute_shorter
    ],
    'SD': [
        Î¾_acute_samples.std(),
        Î¾_chronic_samples.std(),
        Î”Î¾_samples.std(),
        Î²_Î¾_samples.std(),
        np.nan
    ],
    'HDI_2.5%': [
        np.percentile(Î¾_acute_samples, 2.5),
        np.percentile(Î¾_chronic_samples, 2.5),
        np.percentile(Î”Î¾_samples, 2.5),
        np.percentile(Î²_Î¾_samples, 2.5),
        np.nan
    ],
    'HDI_97.5%': [
        np.percentile(Î¾_acute_samples, 97.5),
        np.percentile(Î¾_chronic_samples, 97.5),
        np.percentile(Î”Î¾_samples, 97.5),
        np.percentile(Î²_Î¾_samples, 97.5),
        np.nan
    ]
}

results_df = pd.DataFrame(results_summary)
results_path = output_dir / 'results_v3_6_with_ratios.csv'
results_df.to_csv(str(results_path), index=False)
print(f"âœ… Saved results: {results_path}")

# Save posterior predictive comparison
ppc_summary = {
    'condition': ['Acute', 'Chronic', 'Control'],
    'NAA_pred': [naa_acute_pred.mean(), naa_chronic_pred.mean(), naa_control_pred.mean()],
    'NAA_obs': [naa_obs_acute.mean(), naa_obs_chronic.mean(), naa_obs_control.mean()],
    'n_obs': [len(naa_obs_acute), len(naa_obs_chronic), len(naa_obs_control)]
}
ppc_df = pd.DataFrame(ppc_summary)
ppc_path = output_dir / 'posterior_predictive_comparison.csv'
ppc_df.to_csv(str(ppc_path), index=False)
print(f"âœ… Saved posterior predictive: {ppc_path}")

print("\n" + "=" * 80)
print("âœ… ANALYSIS COMPLETE WITH RATIO DATA INCORPORATED")
print("=" * 80)

print(f"\nðŸŽ¯ FINAL SAMPLE SIZES:")
print(f"   Total acute observations: n={len(naa_obs_acute)}")
if n_acute_valcour > 0:
    print(f"   - Valcour individuals: {n_acute_valcour}")
if not young_bg_acute.empty:
    print(f"   - Young 2014: {int(young_bg_acute['n'].values[0])}")
if not sailasuta_bg_acute.empty:
    print(f"   - Sailasuta 2012: {int(sailasuta_bg_acute['n'].values[0])}")

print(f"\nðŸŽ¯ KEY FINDING:")
print(f"   P(Î¾_acute < Î¾_chronic) = {P_acute_shorter:.4f}")
print(f"   Î”Î¾ = {Î”Î¾_samples.mean():.3f} Â± {Î”Î¾_samples.std():.3f} nm")
print(f"   Î²_Î¾ = {Î²_Î¾_samples.mean():.2f} Â± {Î²_Î¾_samples.std():.2f}")

print("\n" + "=" * 80)